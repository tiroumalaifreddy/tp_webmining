{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-02 15:29:50--  https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/lesechos.json\n",
      "Résolution de people.irisa.fr (people.irisa.fr)… 131.254.254.107\n",
      "Connexion à people.irisa.fr (people.irisa.fr)|131.254.254.107|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 8276509 (7,9M) [application/json]\n",
      "Enregistre : «lesechos.json.1»\n",
      "\n",
      "lesechos.json.1     100%[===================>]   7,89M  6,20MB/s    ds 1,3s    \n",
      "\n",
      "2023-03-02 15:29:52 (6,20 MB/s) - «lesechos.json.1» enregistré [8276509/8276509]\n",
      "\n",
      "--2023-03-02 15:29:53--  https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/sources.json\n",
      "Résolution de people.irisa.fr (people.irisa.fr)… 131.254.254.107\n",
      "Connexion à people.irisa.fr (people.irisa.fr)|131.254.254.107|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 347 [application/json]\n",
      "Enregistre : «sources.json.1»\n",
      "\n",
      "sources.json.1      100%[===================>]     347  --.-KB/s    ds 0s      \n",
      "\n",
      "2023-03-02 15:29:53 (44,1 MB/s) - «sources.json.1» enregistré [347/347]\n",
      "\n",
      "Requirement already satisfied: feedparser in /home/ensai/.local/lib/python3.8/site-packages (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /home/ensai/.local/lib/python3.8/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: newspaper3k in /home/ensai/.local/lib/python3.8/site-packages (0.2.8)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (4.9.2)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /usr/lib/python3/dist-packages (from newspaper3k) (7.0.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/ensai/.local/lib/python3.8/site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/lib/python3/dist-packages (from newspaper3k) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /home/ensai/.local/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (4.56.0)\n",
      "Requirement already satisfied: joblib in /home/ensai/.local/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: click in /home/ensai/.local/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ensai/.local/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (2022.10.31)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/ensai/.local/lib/python3.8/site-packages (from tldextract>=2.0.1->newspaper3k) (3.9.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/ensai/.local/lib/python3.8/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.14.0)\n",
      "Requirement already satisfied: sgmllib3k in /home/ensai/.local/lib/python3.8/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ensai/.local/lib/python3.8/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!wget https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/lesechos.json\n",
    "!wget https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/sources.json\n",
    "!pip install feedparser\n",
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fr-core-news-md==3.5.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.5.0/fr_core_news_md-3.5.0-py3-none-any.whl in /home/ensai/.local/lib/python3.8/site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/ensai/.local/lib/python3.8/site-packages (from fr-core-news-md==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (4.56.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.23.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (45.2.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/ensai/.local/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ensai/.local/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ensai/.local/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ensai/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ensai/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ensai/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.7.9)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guerre en Ukraine en direct : pour la première fois depuis le début de l’invasion de l’Ukraine, des échanges de tirs ont été signalés sur le sol russe Thu, 02 Mar 2023 14:13:47 +0100 https://www.lemonde.fr/international/live/2023/03/02/guerre-en-ukraine-en-direct-la-russie-denonce-une-attaque-terroriste-de-saboteurs-ukrainiens-dans-la-region-de-briansk-kiev-evoque-une-provocation-deliberee-de-moscou_6163832_3210.html\n",
      "Les débats sur la réforme des retraites démarrent au Sénat, Attal déclare « qu’ici il n’y a pas de ZAD, il n’y a que la République » Thu, 02 Mar 2023 16:23:49 +0100 https://www.lemonde.fr/politique/article/2023/03/02/reforme-des-retraites-les-debats-demarrent-au-senat-dans-une-atmosphere-plus-sereine_6163902_823448.html\n",
      "Les dessous du voyage de Marine Le Pen au Sénégal Thu, 02 Mar 2023 06:02:24 +0100 https://www.lemonde.fr/politique/article/2023/03/02/les-dessous-du-voyage-de-marine-le-pen-au-senegal_6163837_823448.html\n",
      "Service national universel : la possible généralisation du programme suscite l’émoi Thu, 02 Mar 2023 16:17:09 +0100 https://www.lemonde.fr/societe/article/2023/03/02/snu-emois-autour-d-une-possible-generalisation-du-programme_6163900_3224.html\n",
      "Réseaux sociaux : à quoi fait référence la « majorité numérique », en débat à l’Assemblée nationale ? Thu, 02 Mar 2023 10:44:34 +0100 https://www.lemonde.fr/pixels/article/2023/03/02/a-quoi-fait-reference-la-majorite-numerique-en-debat-a-l-assemblee-nationale_6163860_4408996.html\n",
      "« Le problème des inégalités de genre ne peut se résoudre que lorsqu’un profond changement de mentalité a lieu » Thu, 02 Mar 2023 04:00:07 +0100 https://www.lemonde.fr/economie/article/2023/03/02/eliza-reid-le-probleme-des-inegalites-de-genre-ne-peut-se-resoudre-que-lorsqu-un-profond-changement-de-mentalite-a-lieu_6163814_3234.html\n",
      "Réforme des retraites : comment fonctionne le régime spécial des sénateurs ? Thu, 02 Mar 2023 07:00:09 +0100 https://www.lemonde.fr/politique/article/2023/03/02/reforme-des-retraites-comment-fonctionne-le-regime-special-des-senateurs_6163842_823448.html\n",
      "Réforme des retraites : « Un acquis de la mobilisation exceptionnelle aura été de nous rappeler la contribution essentielle des syndicats à la démocratie » Thu, 02 Mar 2023 14:30:13 +0100 https://www.lemonde.fr/idees/article/2023/03/02/reforme-des-retraites-un-acquis-de-la-mobilisation-exceptionnelle-aura-ete-de-nous-rappeler-la-contribution-essentielle-des-syndicats-a-la-democratie_6163890_3232.html\n",
      "Le Français Benjamin Brière, acquitté en appel à la mi-février, est toujours emprisonné en Iran Thu, 02 Mar 2023 16:01:23 +0100 https://www.lemonde.fr/international/article/2023/03/02/le-francais-benjamin-briere-acquitte-en-appel-a-la-mi-fevrier-est-toujours-emprisonne-en-iran_6163898_3210.html\n",
      "L’inflation en zone euro persiste à un très haut niveau Thu, 02 Mar 2023 15:08:30 +0100 https://www.lemonde.fr/economie/article/2023/03/02/l-inflation-en-zone-euro-persiste-a-un-tres-haut-niveau_6163892_3234.html\n",
      "En Turquie, malgré le séisme et ses conséquences, le président Erdogan maintient les élections prévues en mai Thu, 02 Mar 2023 11:00:04 +0100 https://www.lemonde.fr/international/article/2023/03/02/en-turquie-malgre-le-seisme-et-ses-consequences-erdogan-maintient-les-elections_6163863_3210.html\n",
      "Accident de trains en Grèce : le chef de gare « avoue une erreur », le gouvernement fait son mea culpa Thu, 02 Mar 2023 13:53:32 +0100 https://www.lemonde.fr/international/article/2023/03/02/accident-de-trains-en-grece-le-chef-de-gare-avoue-une-erreur-le-gouvernement-fait-son-mea-culpa_6163885_3210.html\n",
      "En Israël, la contestation de la réforme de la justice rejoint la critique des violences commises par des colons à Huwara Thu, 02 Mar 2023 12:30:12 +0100 https://www.lemonde.fr/international/article/2023/03/02/en-israel-la-contestation-de-la-reforme-de-la-justice-rejoint-la-critique-des-violences-commises-par-des-colons-a-huwara_6163882_3210.html\n",
      "Gastronomie : à Copenhague, Noma ferme, Geranium pousse Thu, 02 Mar 2023 06:02:20 +0100 https://www.lemonde.fr/m-styles/article/2023/03/02/gastronomie-a-copenhague-noma-ferme-geranium-pousse_6163836_4497319.html\n",
      "Origines du Covid-19 : pourquoi il faut rester prudent Thu, 02 Mar 2023 09:27:10 +0100 https://www.lemonde.fr/les-decodeurs/article/2023/03/02/origines-du-covid-19-pourquoi-il-faut-rester-prudent_6163852_4355770.html\n",
      "Emmanuel Macron au Gabon : l’ère de la « Françafrique est révolue », déclare le président français Thu, 02 Mar 2023 08:52:06 +0100 https://www.lemonde.fr/afrique/article/2023/03/02/emmanuel-macron-a-commence-sa-tournee-africaine-sur-le-theme-de-la-protection-des-forets-au-gabon_6163848_3212.html\n",
      "Filière bois : les autorités françaises dans le piège du « chantage à l’emploi » Wed, 01 Mar 2023 18:06:43 +0100 https://www.lemonde.fr/les-decodeurs/article/2023/03/01/filiere-bois-les-autorites-francaises-dans-le-piege-du-chantage-a-l-emploi_6163783_4355770.html\n",
      "Un an après son adoption, la loi sport à l’épreuve de la crise de gouvernance du mouvement sportif français Thu, 02 Mar 2023 09:00:04 +0100 https://www.lemonde.fr/sport/article/2023/03/02/un-an-apres-son-adoption-la-loi-sport-a-l-epreuve-de-la-crise-de-gouvernance-du-mouvement-sportif-francais_6163849_3242.html\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "import json\n",
    "f = open('lesechos.json')\n",
    "data_lesechos = json.load(f)\n",
    "\n",
    "# (a) se premunir contre le bloquage de commande\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# (b) recuperer le fichier RSS\n",
    "import feedparser as fp\n",
    "url = \"https://www.lemonde.fr/rss/une.xml\"\n",
    "data = fp.parse(url)\n",
    "\n",
    "for item in data.entries:\n",
    "    print(item.title, item.published, item.link)\n",
    "\n",
    "f = open('sources.json')\n",
    "liste_rss_flux = json.load(f)\n",
    "liste_rss_flux = list(liste_rss_flux.values())\n",
    "\n",
    "\n",
    "liste_new_articles = []\n",
    "# In lesechos.json, links are the keys but we cannot compare these links to the link we get when we parse RSS links.\n",
    "# For example, a link from lesechos.json is \n",
    "# https://investir.lesechos.fr/marches/actualites/analyse-technique-toute-poursuite-de-la-consolidation-du-contrat-cac-40-sera-limitee-1995443.php?xtor=RSS-24\n",
    "# While a link from an item entry is \n",
    "# https://investir.lesechos.fr/actu-des-valeurs/la-vie-des-actions/la-normalisation-des-ventes-de-velo-apres-le-boom-du-covid-laisse-des-traces-dans-les-comptes-de-thule-1905607\n",
    "# So we need to compare only /marches/actualites/analyse-technique-toute-poursuite-de-la-consolidation-du-contrat-cac-40-sera-limitee\n",
    "liste_url_article_already_in_base = [urlparse(temp).path[:-12] for temp in list(data_lesechos.keys())]\n",
    "for url in liste_rss_flux:\n",
    "    data_rss = fp.parse(url)\n",
    "    for item in data_rss.entries:\n",
    "        link = item.link\n",
    "        res = urlparse(link)\n",
    "        if res.path[:-8] not in liste_url_article_already_in_base:\n",
    "            liste_new_articles.append(link)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour l’un des articles de votre flux RSS, regardez le code HTML de la page web. Pouvez-vous\n",
    "identifier les zones contenant le titre de l’article ? le texte de l’article ?**\n",
    "\n",
    "En analysant le code HTML d'un article sur `lesechos.fr`, on remarque que les balises pour le titre et le texte d'un article  ne sont pas clairement identifiables. On a par exemple des balises pour les titres au format suivant : `h1 class=\"sc-14kwckt-6 sc-14omazk-0 jtuxZx fsUfny\"`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que faudrait-il faire pour récupérer le texte de l’article ? En quoi le recours à la librairie newspaper est-il utile ?**\n",
    "\n",
    "Utiliser ```newspaper``` nous simplifie la tâche : on peut accéder facilement aux élements de la page qui nous intéressent. De plus ce package est optimisé pour les articles de presse alors qu'un package comme BeautifulSoup serait plus contraignant ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La normalisation des ventes de vélo, après le boom du Covid, laisse des traces dans les comptes de Thule []\n",
      "Par Marjorie Encelot\n",
      "\n",
      "Publié le 10 févr. 2023 à 15:31 Mis à jour le 10 févr. 2023 à 15:57\n",
      "\n",
      "L’année 2021, pendant la crise sanitaire, a été « exceptionnelle » pour le vélo. Trop, finalement, quand à l’instar du suédois Thule Group (-17% à la Bourse de Stockholm, plus forte baisse aujourd’hui de l’indice européen Stoxx 600), on est une entreprise cotée en Bourse, qui vend des porte-vélos et des remorques pour vélo, et que la base de comparaison est historiquement élevée au moment de présenter ses \n",
      "https://media.lesechos.com/api/v1/images/view/63e653ee6850761e8425f4db/1280x720/thule-2.jpg\n"
     ]
    }
   ],
   "source": [
    "import newspaper as np\n",
    "\n",
    "url_one_article = liste_new_articles[0]\n",
    "\n",
    "article = np.Article('https://investir.lesechos.fr/actu-des-valeurs/la-vie-des-actions/la-normalisation-des-ventes-de-velo-apres-le-boom-du-covid-laisse-des-traces-dans-les-comptes-de-thule-1905607')\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "print(article.title, article.authors) # Titre de l'article\n",
    "print(article.text[:500]) # Extrait de l'article\n",
    "print(article.top_image) # Image principale de l'article"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reprendre votre programme de scrapping et le compléter de manière à mettre à jour votre\n",
    "base de données avec le texte des articles du flux RSS et le lien vers l’image illustrant l’article\n",
    "lorsqu’il y en a une. Vous pouvez inclure d’autres informations qui vous semblent utile pour un\n",
    "traitement ultérieur.**\n",
    "\n",
    "On traverse la liste des articles et on extrait les informations à l'aide de `newspaper` comme dans la question 2. Pour chaque article on ajoute les informations suivantes :\n",
    "- le titre\n",
    "- la date\n",
    "- les auteurs\n",
    "- le texte\n",
    "- le lien de l'image principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for url in liste_new_articles:\n",
    "    article = np.Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    temp_dict = {'title' : article.title, 'date' : article.publish_date.strftime(\"%m/%d/%Y, %H:%M:%S\"), 'author' : article.authors, 'category' : None, 'content' : article.text, 'image_link' : article.top_image, 'image_file' : None}\n",
    "    data_lesechos[url] = temp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lesechos_updated.json\", \"w\") as outfile:\n",
    "    json.dump(data_lesechos, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de télécharger des images depuis une url en python de la manière suivante :\n",
    "> ```python\n",
    "> import requests\n",
    "> \n",
    "> img_data = requests.get(image_url).content\n",
    "> with open('image_name.jpg', 'wb') as outfile:\n",
    ">   outfile.write(img_data)\n",
    "> ```\n",
    "\n",
    "Pour préserver les images des articles, on pourrait les copier lorsque l'on réalise le scraping du texte du site. On sauvegarderait alors toutes les images dans un meme dossier `articles_img` en précisant le nom de l'article pour chaque image.\n",
    "\n",
    "Pour simplifier le rangement et l'organisation des images, il serait possible de les ranger dans des dossiers par date ou par journal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Donnez une idée d’une (ou plusieurs) autre(s) source(s) que l’on pourrait-on écouter pour col-\n",
    "lecter des informations complémentaires sur ces articles ? Sans donner le code, donnez une idée\n",
    "de ce que l’on pourrait collecter sur ces sources et de la/des librairie(s) qui pourrai(en)t nous\n",
    "aider en cela.**\n",
    "\n",
    "Nous pouvons regarder dans les médias sociaux tels que Twitter. Par exemple, nous pourrions suivre le profil du journal et pour un article (au cas où ils tweetent au sujet de leur article) regarder certaines statistiques telles que les vues, RT, etc...\n",
    "\n",
    "Pour facilement récupérer les données sur twitter on peut utiliser la libraire `tweepy`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À quoi correspondent les étiquettes IOB utilisées ?**\n",
    "\n",
    "Les étiquettes IOB (pour Inside–outside–beginning) correspondent au type d'entité nommée. \n",
    "\n",
    "Par exemple `emmanuel` sera étiquetté en `PER` et `paris` en `LOC`. \n",
    "\n",
    "\n",
    "**Expliquer brièvement une technique (description de la tâche, du modèle) pour la détection des entités nommées.**\n",
    "\n",
    "Pour construire une méthode de détection d'entités nommées on utilise un jeu de données disposant de mots déjà étiquetés.\n",
    "\n",
    "On peut ensuite entrainer plusieurs types de modèles sur ce jeu de données. On pourrait par exemple entrainer un réseau de neurone avec une couche LSTM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Écrire un programme qui traite l’ensemble des textes que vous aurez chargé depuis le fichier\n",
    "lesechos.json et stock pour chaque article les entités trouvées et les informations afférentes\n",
    "(type, span).**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En plus de stocker les entités par document, on stocke également les entités par phrase. Pour un document on aura donc une liste de listes( = phrase), et dans chaque sous liste on aura une liste d'entités avec leurs informations. Ce travail en amont nous sera utile pour la question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "data_lesechos_updated = json.load(open('lesechos_updated.json'))\n",
    "list_text = [temp['content'] for temp in list(data_lesechos_updated.values())]\n",
    "nlp = spacy.load('fr_core_news_md', disable=['tagging', 'parser'])\n",
    "nlp.add_pipe('sentencizer')\n",
    "docs = list(nlp.pipe(list_text))\n",
    "list_entites_per_doc = []\n",
    "for doc in docs:\n",
    "    temp = []\n",
    "    for s in doc.sents:\n",
    "        temp_sentence = []\n",
    "        for entity in s.ents:\n",
    "            temp_sentence.append({'text' : entity, 'label' : entity.label_, 'start' : entity.start, 'end' : entity.end})\n",
    "        temp.append(temp_sentence)\n",
    "    list_entites_per_doc.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': Etats, 'label': 'LOC', 'start': 66, 'end': 67}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_entites_per_doc[0][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour chaque type d’entité, déterminer les 20 plus fréquentes dans la collection de documents.\n",
    "Commentez le résultat obtenu. On mémorisera ces entités dans trois listes distinctes (une par\n",
    "type d’entité).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "flat_list = list(itertools.chain(*list_entites_per_doc))\n",
    "flat_list = list(itertools.chain(*flat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def takeFirst(elem):\n",
    "    return elem[0]\n",
    "\n",
    "label_counts = defaultdict(int)\n",
    "for item in flat_list:\n",
    "    label_counts[(str(item['text']), str(item['label']))] += 1\n",
    "\n",
    "result = defaultdict(list)\n",
    "for key, count in label_counts.items():\n",
    "    result[key[1]].append((count, key[0]))\n",
    "\n",
    "for label, items in result.items():\n",
    "    items.sort(key = takeFirst ,reverse=True)\n",
    "    result[label] = items[:20]\n",
    "\n",
    "list_PER = result['PER']\n",
    "list_ORG = result['ORG']\n",
    "list_LOC = result['LOC']\n",
    "list_MISC = result['MISC']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compte pour chaque entité (i.e. le \"texte\") le nombre d'occurences et on trie en fonction de ce nombre."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Écrire une fonction qui prend en entrée une paire d’entités et les types correspondant et retourne\n",
    "le nombre de co-occurrences de ces deux entités au sein d’un même document dans la collection.**\n",
    "\n",
    "L'argument `same_sentence` permet ici de préciser le cas où on cherche les occurences dans une même phrase (cf question 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_co_occurence(pair_entities : tuple, types : tuple, same_sentence : bool = False):\n",
    "    first_entity, second_entity = pair_entities\n",
    "    first_type, second_type = types\n",
    "    count = 0\n",
    "    for doc in list_entites_per_doc:\n",
    "        if same_sentence == False:\n",
    "            indice_first = False\n",
    "            indice_second = False\n",
    "            doc = list(itertools.chain(*doc))\n",
    "            for entity in doc:\n",
    "                if str(entity['text']) == str(first_entity) and str(entity['label']) == str(first_type):\n",
    "                    indice_first = True\n",
    "                if str(entity['text']) == str(second_entity) and str(entity['label']) == str(second_type):\n",
    "                    indice_second = True    \n",
    "            if indice_first and indice_second:\n",
    "                count += 1\n",
    "        if same_sentence:\n",
    "            for sentence in doc:\n",
    "                indice_first = False\n",
    "                indice_second = False\n",
    "                for entity in sentence:\n",
    "                    if str(entity['text']) == str(first_entity) and str(entity['label']) == str(first_type):\n",
    "                        indice_first = True\n",
    "                    if str(entity['text']) == str(second_entity) and str(entity['label']) == str(second_type):\n",
    "                        indice_second = True    \n",
    "                if indice_first and indice_second:\n",
    "                    count += 1\n",
    "\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_co_occurence(('Omicron', 'Cac 40'), ('PER', 'MISC'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import permutations\n",
    " \n",
    "# initialize lists\n",
    "list_per = [{'text' : e[1], 'label' : 'PER'} for e in list_PER]\n",
    "list_org = [{'text' : e[1], 'label' : 'ORG'} for e in list_ORG]\n",
    "list_loc = [{'text' : e[1], 'label' : 'LOC'} for e in list_LOC]\n",
    "list_misc = [{'text' : e[1], 'label' : 'MISC'} for e in list_MISC]\n",
    "\n",
    "big_list = list_per + list_org + list_loc + list_misc\n",
    "\n",
    "combinations = []\n",
    "for pair in itertools.combinations(big_list, 2):\n",
    "    if pair[0]['label'] != pair[1]['label']:\n",
    "        combinations.append([(pair[0]['text'], pair[1]['text']), (pair[0]['label'], pair[1]['label'])])\n",
    "\n",
    "final = [list(pair) for pair in combinations]\n",
    "\n",
    "list_occu = []\n",
    "for l in final:\n",
    "    first_entity, second_entity = l[0]\n",
    "    first_type, second_type = l[1]\n",
    "    list_occu.append((nb_co_occurence((first_entity, second_entity), (first_type, second_type)), l[0], l[1]))\n",
    "\n",
    "list_occu = sorted(list_occu, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(578, ('Etats-Unis', 'Cac 40'), ('LOC', 'MISC')),\n",
       " (551, ('Fed', 'Cac 40'), ('ORG', 'MISC')),\n",
       " (533, ('Fed', 'Etats-Unis'), ('ORG', 'LOC')),\n",
       " (314, ('Etats-Unis', 'Dow Jones'), ('LOC', 'MISC')),\n",
       " (306, ('Jerome Powell', 'Fed'), ('PER', 'ORG')),\n",
       " (287, ('Etats-Unis', 'Bourse de Paris'), ('LOC', 'MISC')),\n",
       " (284, ('Fed', 'Dow Jones'), ('ORG', 'MISC')),\n",
       " (281, ('Fed', 'Bourse de Paris'), ('ORG', 'MISC')),\n",
       " (277, ('Reuters', 'Etats-Unis'), ('ORG', 'LOC')),\n",
       " (276, ('Fed', 'S&P 500'), ('ORG', 'MISC'))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_occu[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import permutations\n",
    " \n",
    "# initialize lists\n",
    "list_per = [{'text' : e[1], 'label' : 'PER'} for e in list_PER]\n",
    "list_org = [{'text' : e[1], 'label' : 'ORG'} for e in list_ORG]\n",
    "list_loc = [{'text' : e[1], 'label' : 'LOC'} for e in list_LOC]\n",
    "list_misc = [{'text' : e[1], 'label' : 'MISC'} for e in list_MISC]\n",
    "\n",
    "big_list = list_per + list_org + list_loc + list_misc\n",
    "\n",
    "combinations = []\n",
    "for pair in itertools.combinations(big_list, 2):\n",
    "    if pair[0]['label'] != pair[1]['label']:\n",
    "        combinations.append([(pair[0]['text'], pair[1]['text']), (pair[0]['label'], pair[1]['label'])])\n",
    "\n",
    "final = [list(pair) for pair in combinations]\n",
    "\n",
    "list_occu = []\n",
    "for l in final:\n",
    "    first_entity, second_entity = l[0]\n",
    "    first_type, second_type = l[1]\n",
    "    list_occu.append((nb_co_occurence((first_entity, second_entity), (first_type, second_type), same_sentence=True), l[0], l[1]))\n",
    "\n",
    "list_occu = sorted(list_occu, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(578, ('Etats-Unis', 'Cac 40'), ('LOC', 'MISC')),\n",
       " (551, ('Fed', 'Cac 40'), ('ORG', 'MISC')),\n",
       " (533, ('Fed', 'Etats-Unis'), ('ORG', 'LOC')),\n",
       " (314, ('Etats-Unis', 'Dow Jones'), ('LOC', 'MISC')),\n",
       " (306, ('Jerome Powell', 'Fed'), ('PER', 'ORG')),\n",
       " (287, ('Etats-Unis', 'Bourse de Paris'), ('LOC', 'MISC')),\n",
       " (284, ('Fed', 'Dow Jones'), ('ORG', 'MISC')),\n",
       " (281, ('Fed', 'Bourse de Paris'), ('ORG', 'MISC')),\n",
       " (277, ('Reuters', 'Etats-Unis'), ('ORG', 'LOC')),\n",
       " (276, ('Fed', 'S&P 500'), ('ORG', 'MISC'))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_occu[:10] # Les 10 occurences les plus fréquentes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ecrivez une fonction qui prend en entrée deux entités et leurs types\n",
    "respectif et qui analyse l’ensemble des documents où ces entités apparaissent dans la même\n",
    "phrase de manière à établir la liste des verbes qui apparaissent entre ces deux entités dans ces\n",
    "phrases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbs_between_entities(pair_entities : tuple, types : tuple):\n",
    "    first_entity, second_entity = pair_entities\n",
    "    first_type, second_type = types\n",
    "    list_verbs = []\n",
    "    for doc in list_entites_per_doc:\n",
    "        for sentence in doc:\n",
    "            indice_first = False\n",
    "            indice_second = False\n",
    "            for entity in sentence:\n",
    "                if str(entity['text']) == str(first_entity) and str(entity['label']) == str(first_type):\n",
    "                    indice_first = True\n",
    "                    start_first = entity['start']\n",
    "                    end_first = entity['end']\n",
    "                if str(entity['text']) == str(second_entity) and str(entity['label']) == str(second_type):\n",
    "                    indice_second = True\n",
    "                    start_second = entity['start']\n",
    "                    end_second = entity['end']    \n",
    "            if indice_first and indice_second:\n",
    "                start = min(start_first, start_second)\n",
    "                end = max(end_first, end_second)\n",
    "                true_sentence = sentence[0]['text'].sent\n",
    "                portion = true_sentence[start:end]\n",
    "                for token in portion:\n",
    "                    if str(token.pos_) == 'VERB':\n",
    "                        list_verbs.append(token)\n",
    "    return list_verbs\n",
    "                \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En utilisant la fonction de la question précédente, déterminez quelques couples d’entités et un\n",
    "verbe associé qui vous paraissent faire sens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gagne,\n",
       " ancrés,\n",
       " connu,\n",
       " finit,\n",
       " dévissent,\n",
       " évolue,\n",
       " retombé,\n",
       " poursuivi,\n",
       " prononcée,\n",
       " poursuit,\n",
       " poursuit]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_between_entities(('Wall Street', 'Cac 40'), ('LOC', 'MISC'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les verbes `gagner`, `évoluer` ou encore `dévisser` font sens pour la relation `Wall Street` et `CAC40` puisque l'on utilise ces verbes pour parler des quotations boursières."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[annoncé,\n",
       " vont,\n",
       " puiser,\n",
       " annoncé,\n",
       " annoncé,\n",
       " trouvé,\n",
       " comprenant,\n",
       " signé,\n",
       " imposant,\n",
       " voulant,\n",
       " rendre,\n",
       " signé,\n",
       " portant,\n",
       " écartant,\n",
       " parvenus]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_between_entities(('Joe Biden', 'Etats-Unis'), ('PER', 'LOC'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le verbe `annoncer` semble faire sens pour la relation `Joe Biden` et `Etats-Unis` puisque le président annonce des lois ou des actions gouvernementales qui concernent directement son pays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pourrait, conduire, alimentant, examine, évaluer, explique, prévu, concernant]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_between_entities(('Fed', 'Etats-Unis'), ('ORG', 'LOC'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les verbes `examiner` et `évaluer` font sens pour la relation entre `Fed` et `Etats-Unis` puisque la Fed s'occupe de surveiller plusieurs indicateurs aux Etats-Unis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En quoi les couples et verbes identifés à la question précédente peuvent-ils être utiles pour\n",
    "trouver de nouvelles relations et/ou grouper des relations de même nature ?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourrait trouver de nouvelles relations à l'aide des relations communes. Par exemple :\n",
    "\n",
    "Si `Joe Biden` est lié à `Etats-Unis` qui est lui meme lié à `Fed`. On pourrait alors rechercher un lien entre `Joe Biden` et `Fed`.\n",
    "\n",
    "De la même manière, on pourrait dérouler ces liens jusqu'à trouver des ensembles cycliques, par exemple :\n",
    "\n",
    "`Wall Street` -> `Dow Jones` -> `S&P 500` -> `Fed` -> `Wall Street`\n",
    "\n",
    "Cette ensemble pourrait correspondre à un groupe et etre associé à une thématique, par exemple ici la bourse. Les verbes identifiés serait alors utiles pour mieux comprendres les thématiques et les intéractions dans les groupes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment pourrait-on affiner cette analyse en utilisant l’arbre de dépendance syntaxique ?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
